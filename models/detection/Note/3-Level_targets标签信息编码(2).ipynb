{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重点！\n",
    "* **目标检测的关键部分 -- 样本（Anchor与GT）匹配**\n",
    "* **样本匹配是区分不同目标检测系列的另一个关键部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor & Feature map**\n",
    "\n",
    "Tensor:(h, w, c)=(2,5,3) ;\n",
    "\n",
    "Feature map:(b, h, w, c)=(3,2,4,5)\n",
    "\n",
    "<img src=\"imgs/3-axis_front.png\" width=\"400\" height=\"400\" align=\"left\">\n",
    "\n",
    "<img src=\"imgs/4-axis_block.png\" width=\"300\" height=\"400\" align=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 假设一张输入图片的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 一个标签框的情况 \n",
    "# gt_boxes = np.array([[[10, 12, 48, 60]]])\n",
    "# 一个gt_box对应的类别标签\n",
    "# classes = np.array([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of gt_box in one img: 2\n",
      "gt_boxes: torch.Size([1, 2, 4]) torch.float32\n",
      "classes: torch.Size([1, 2]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 当前图片中有两个标签框的情况，每个框由4个坐标表示，设定batch_size=1,\n",
    "gt_boxes = np.array([[[10, 12, 48, 60],\n",
    "                      [11, 20, 40, 50]]])\n",
    "# 这两个gt_box各自对应的类别\n",
    "classes = np.array([[2,4]])  # （batch_size, num_of_gtbox)\n",
    "m = gt_boxes.shape[1]; print('num of gt_box in one img:',m)  # 标签框个数\n",
    "\n",
    "# 转变numpy的ndarray的数据类型，到torch的tensor数据类型，\n",
    "gt_boxes = torch.from_numpy(gt_boxes)\n",
    "gt_boxes = gt_boxes.type(torch.float32)  # 转换张量的数值类型，方便后续计算\n",
    "classes = torch.from_numpy(classes)\n",
    "\n",
    "print('gt_boxes:', gt_boxes.shape, gt_boxes.dtype)\n",
    "print('classes:', classes.shape, classes.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这张图片输入模型后，模型输出的预测值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**假设网络有三层输出(head的其中三层), (h,w)大小依次为 （16，16） （8，8） （4，4）**\n",
    "\n",
    "batch_size = 1；  num_of_class = 5（即图中classification的通道数C=5）\n",
    "<img src=\"imgs/head_demo.png\" width=\"500\" height=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有的分类层\n",
    "cls_logits_all = [random.rand(1, 5, 16, 16),  # 第一层\n",
    "                 random.rand(1, 5, 8, 8),     # 第二层\n",
    "                 random.rand(1, 5, 4, 4),]    # 第三层\n",
    "# 所有的中心度层\n",
    "cnt_logits_all = [random.rand(1, 1, 16, 16),  # 第一层\n",
    "                 random.rand(1, 1, 8, 8),     # 第二层\n",
    "                 random.rand(1, 1, 4, 4),]    # 第三层\n",
    "# 所有的回归层\n",
    "reg_preds_all = [random.rand(1, 4, 16, 16),   # 第一层\n",
    "                 random.rand(1, 4, 8, 8),     # 第二层\n",
    "                 random.rand(1, 4, 4, 4),]    # 第三层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 步骤：\n",
    "1. feature Map 对应回原图\n",
    "2. 求匹配矩阵（anchor - target）\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征图信息编码（映射/转换）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征图上格点坐标 与 原图坐标 的映射(对应)关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "feature_map_hw: torch.Size([4, 4])\n",
      "class_num: 5\n",
      "torch.Size([1, 4, 4, 5]) torch.float64\n"
     ]
    }
   ],
   "source": [
    "# 以第三层的运算为例 看一下第三个输出层中分类分支特征图的数据形式\n",
    "cls_logits, cnt_logits, reg_preds = cls_logits_all[2], cnt_logits_all[2], reg_preds_all[2]\n",
    "\n",
    "cls_logits = torch.from_numpy(cls_logits)  # 将numpy格式数据转换成tensor\n",
    "cls_logits = cls_logits.permute(0, 2, 3, 1)  # [batch_size,h,w,class_num]\n",
    "\n",
    "batch_size = cls_logits.shape[0] ;print('batch_size:', batch_size)\n",
    "hw = cls_logits.shape[1:3]; print('feature_map_hw:', hw)  # 图像的高宽\n",
    "class_num = cls_logits.shape[3] ;print('class_num:', class_num)\n",
    "print(cls_logits.shape, cls_logits.dtype)  # 特征图上每一个grid是一个5维的向量  用来表示当前点的类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/shape.png\" width=\"300\" height=\"400\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.,  8.],\n",
       "        [24.,  8.],\n",
       "        [40.,  8.],\n",
       "        [56.,  8.],\n",
       "        [ 8., 24.],\n",
       "        [24., 24.],\n",
       "        [40., 24.],\n",
       "        [56., 24.],\n",
       "        [ 8., 40.],\n",
       "        [24., 40.],\n",
       "        [40., 40.],\n",
       "        [56., 40.],\n",
       "        [ 8., 56.],\n",
       "        [24., 56.],\n",
       "        [40., 56.],\n",
       "        [56., 56.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coords_fmap2orig(feature, stride=16):  # 特征图上的grid对应于原图的位置\n",
    "    '''\n",
    "    transfer one fmap coords to orig coords\n",
    "    Args:\n",
    "        feature -- [batch_size,h,w,c]: 输出的特征图\n",
    "        stride -- int: 特征图相对原图的下采样倍数\n",
    "    Returns ：\n",
    "        coords [n,2]\n",
    "    '''\n",
    "    h, w = feature.shape[1:3]\n",
    "    # h, w = 8, 8 # 为演示方便，我们使用尺寸为（4，4）的特征图\n",
    "    shifts_x = torch.arange(0, w * stride, stride, dtype=torch.float32) # [ 0., 16., 32., 48.]\n",
    "    shifts_y = torch.arange(0, h * stride, stride, dtype=torch.float32) # [ 0., 16., 32., 48.]\n",
    "    \n",
    "    # meshgrid()生成网格，可以用于生成坐标。\n",
    "    # 函数输入两个数据类型相同的一维张量，两个输出张量的行数为第一个输入张量的元素个数，列数为第二个输入张量的元素个数。\n",
    "    shift_y, shift_x = torch.meshgrid([shifts_y, shifts_x]) \n",
    "    shift_x = torch.reshape(shift_x, [-1])\n",
    "    shift_y = torch.reshape(shift_y, [-1])\n",
    "    coords = torch.stack([shift_x, shift_y], -1) + (stride // 2)  # 中心点偏置 # [ 8., 24., 40., 56.]\n",
    " \n",
    "    # 可视化一下，看看\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.scatter(coords[:,0], coords[:,1])\n",
    "    # plt.savefig('grid.png')\n",
    "    return coords\n",
    "\n",
    "coords = coords_fmap2orig(cls_logits)\n",
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/perception field.png\" width=\"700\" height=\"400\" align=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用形状转换，将特征图降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cls_logits \n",
    "cls_logits = cls_logits.reshape((batch_size, -1, class_num))  # [batch_size,h*w,class_num]\n",
    "cls_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/reshape1.png\" width=\"600\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 reshape cnt\n",
    "cnt_logits = torch.from_numpy(cnt_logits)\n",
    "cnt_logits = cnt_logits.permute(0, 2, 3, 1)\n",
    "cnt_logits = cnt_logits.reshape((batch_size, -1, 1))    # [batch_size,h*w,1]\n",
    "cnt_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 reshape reg\n",
    "reg_preds = torch.from_numpy(reg_preds)\n",
    "reg_preds = reg_preds.permute(0, 2, 3, 1)\n",
    "reg_preds = reg_preds.reshape((batch_size, -1, 4))  #  # [batch_size,h*w,4]\n",
    "reg_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换后特征图的形状特征\n",
    "h_mul_w = cls_logits.shape[1]\n",
    "h_mul_w  # h*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找特征图上的点与gtbox边缘的偏置关系\n",
    "\n",
    "&emsp;&emsp;特征图上feature_grid的坐标（x_fea,y_fea）先映射到原图上(x_img, y_img), 然后找点(x_img, y_img)与gtbox坐标(x_min,y_min, x_max,y_max)的偏置关系:当前点到框的各边的距离(l, t, r, b)\n",
    "\n",
    "<img src=\"imgs/tensor2fea2img.png\" width=\"900\" height=\"400\" align=\"bottom\">\n",
    "\n",
    "* **匹配原则**\n",
    "\n",
    "**1) in box**\n",
    "\n",
    "**2) scale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 24., 40., 56.,  8., 24., 40., 56.,  8., 24., 40., 56.,  8., 24.,\n",
      "        40., 56.])\n",
      "tensor([ 8.,  8.,  8.,  8., 24., 24., 24., 24., 40., 40., 40., 40., 56., 56.,\n",
      "        56., 56.])\n"
     ]
    }
   ],
   "source": [
    "x = coords[:, 0] ; print(x)\n",
    "y = coords[:, 1] ; print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **利用两个角点坐标 分别和 anchor point坐标 相减，判断anchor point是否在目标框内！**\n",
    "* **Xa-Xtl>0 & Ya-Ytl>0 & Xbr-Xa>0 & Ybr-Ya>0 同时满足4个条件，可判定anchor point在GT框内！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[10 12 48 60]\n",
      "  [11 20 40 50]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10, 11]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 假设一张图上有两个gt_box\n",
    "gt_boxes = np.array([[[10, 12, 48, 60],\n",
    "                      [11, 20, 40, 50]]]) \n",
    "print(gt_boxes)\n",
    "gt_boxes[..., 0]  # 取第一列所有gt框的xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10., 11.]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_boxes[..., 0][:, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2., -3.],\n",
      "         [14., 13.],\n",
      "         [30., 29.],\n",
      "         [46., 45.],\n",
      "         [-2., -3.],\n",
      "         [14., 13.],\n",
      "         [30., 29.],\n",
      "         [46., 45.],\n",
      "         [-2., -3.],\n",
      "         [14., 13.],\n",
      "         [30., 29.],\n",
      "         [46., 45.],\n",
      "         [-2., -3.],\n",
      "         [14., 13.],\n",
      "         [30., 29.],\n",
      "         [46., 45.]]]) torch.Size([1, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "# [1,h*w,1]-[batch_size,1,m]-->[batch_size,h*w,m]\n",
    "l_off = x[None, :, None] - gt_boxes[..., 0][:, None, :]   # None 增加维度用的\n",
    "print(l_off, l_off.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/sub.png\" width=\"400\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "t_off = y[None, :, None] - gt_boxes[..., 1][:, None, :]\n",
    "\n",
    "r_off = gt_boxes[..., 2][:, None, :] - x[None, :, None]\n",
    "b_off = gt_boxes[..., 3][:, None, :] - y[None, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 2, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltrb_off = torch.stack([l_off, t_off, r_off, b_off], dim=-1)  # [batch_size,h*w,m,4]\n",
    "ltrb_off.shape # [1,16,2,4]\n",
    "\n",
    "# left/top/right/bottom offset\n",
    "# 16 <-- 4*4 (H*W)\n",
    "# 2 <-- # of GT-Box\n",
    "# 4 <-- 4 offset value:[l_off, t_off, r_off, b_off] ==> 只有当这4个值都大于0， 才可以判定anchor point在GT框内\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -2.,  -4.,  40.,  52.],\n",
       "          [ -3., -12.,  32.,  42.]],\n",
       "\n",
       "         [[ 14.,  -4.,  24.,  52.],\n",
       "          [ 13., -12.,  16.,  42.]],\n",
       "\n",
       "         [[ 30.,  -4.,   8.,  52.],\n",
       "          [ 29., -12.,   0.,  42.]],\n",
       "\n",
       "         [[ 46.,  -4.,  -8.,  52.],\n",
       "          [ 45., -12., -16.,  42.]],\n",
       "\n",
       "         [[ -2.,  12.,  40.,  36.],\n",
       "          [ -3.,   4.,  32.,  26.]],\n",
       "\n",
       "         [[ 14.,  12.,  24.,  36.],\n",
       "          [ 13.,   4.,  16.,  26.]],\n",
       "\n",
       "         [[ 30.,  12.,   8.,  36.],\n",
       "          [ 29.,   4.,   0.,  26.]],\n",
       "\n",
       "         [[ 46.,  12.,  -8.,  36.],\n",
       "          [ 45.,   4., -16.,  26.]],\n",
       "\n",
       "         [[ -2.,  28.,  40.,  20.],\n",
       "          [ -3.,  20.,  32.,  10.]],\n",
       "\n",
       "         [[ 14.,  28.,  24.,  20.],\n",
       "          [ 13.,  20.,  16.,  10.]],\n",
       "\n",
       "         [[ 30.,  28.,   8.,  20.],\n",
       "          [ 29.,  20.,   0.,  10.]],\n",
       "\n",
       "         [[ 46.,  28.,  -8.,  20.],\n",
       "          [ 45.,  20., -16.,  10.]],\n",
       "\n",
       "         [[ -2.,  44.,  40.,   4.],\n",
       "          [ -3.,  36.,  32.,  -6.]],\n",
       "\n",
       "         [[ 14.,  44.,  24.,   4.],\n",
       "          [ 13.,  36.,  16.,  -6.]],\n",
       "\n",
       "         [[ 30.,  44.,   8.,   4.],\n",
       "          [ 29.,  36.,   0.,  -6.]],\n",
       "\n",
       "         [[ 46.,  44.,  -8.,   4.],\n",
       "          [ 45.,  36., -16.,  -6.]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16个(x_img,y_img)点与这个gt_box(x_min,y_min,x_max,y_max)的偏置关系\n",
    "ltrb_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -4., -12.],\n",
       "         [ -4., -12.],\n",
       "         [ -4., -12.],\n",
       "         [ -8., -16.],\n",
       "         [ -2.,  -3.],\n",
       "         [ 12.,   4.],\n",
       "         [  8.,   0.],\n",
       "         [ -8., -16.],\n",
       "         [ -2.,  -3.],\n",
       "         [ 14.,  10.],\n",
       "         [  8.,   0.],\n",
       "         [ -8., -16.],\n",
       "         [ -2.,  -6.],\n",
       "         [  4.,  -6.],\n",
       "         [  4.,  -6.],\n",
       "         [ -8., -16.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(ltrb_off, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择feature map上的正负样本点\n",
    "&emsp;&emsp;对于feature map上的各个点（x_fea,y_fea），只要这个点对应的(x_img, y_img)落到了gt bbox(x_min,y_min, x_max,y_max)区域中，那么这个点就是正样本；而如果这个点多在多个bbox中，那个这个点就是模糊样本，目前采用面积最小的bbox作为这个点的回归目标。目标框的回归参数是：当前点到框的各边的距离(l, t, r, b)。\n",
    "<img src=\"imgs/ancher free.png\" width=\"500\" height=\"400\" align=\"bottom\">\n",
    ">```python\n",
    "l_off = x[None, :, None] - gt_boxes[..., 0][:, None, :]\n",
    "t_off = y[None, :, None] - gt_boxes[..., 1][:, None, :]\n",
    "r_off = gt_boxes[..., 2][:, None, :] - x[None, :, None]\n",
    "b_off = gt_boxes[..., 3][:, None, :] - y[None, :, None]\n",
    "ltrb_off = torch.stack([l_off, t_off, r_off, b_off], dim=-1)  # [batch_size,h*w,m,4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_min = torch.min(ltrb_off, dim=-1)[0]  # [batch_size,h*w,m] 注意：用于验证是否在目标框内\n",
    "off_max = torch.max(ltrb_off, dim=-1)[0]  # [batch_size,h*w,m] 注意：用于限制检测范围 ltbr距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [ True,  True],\n",
       "         [ True, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [ True,  True],\n",
       "         [ True, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [ True, False],\n",
       "         [ True, False],\n",
       "         [False, False]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_min > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [ True,  True],\n",
       "         [ True, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [ True,  True],\n",
       "         [ True, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [ True, False],\n",
       "         [ True, False],\n",
       "         [False, False]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_in_gtboxes = off_min > 0  # 满足条件的为真 1 保留\n",
    "mask_in_gtboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [False,  True],\n",
       "         [False,  True],\n",
       "         [False,  True],\n",
       "         [False, False],\n",
       "         [False,  True],\n",
       "         [ True,  True],\n",
       "         [ True,  True],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [False, False]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_in_level = (off_max > 16) & (off_max <= 32)\n",
    "mask_in_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/scale.png\" width=\"600\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找正样本点与gtbox中心的偏置关系\n",
    "<img src=\"imgs/ancher free.png\" width=\"500\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[29.0000, 25.5000]])\n",
      "tensor([[36., 35.]])\n"
     ]
    }
   ],
   "source": [
    "# 计算gt_box的中心  我们假设有两个gt box \n",
    "gt_center_x = (gt_boxes[..., 0] + gt_boxes[..., 2]) / 2  ; print(gt_center_x)\n",
    "gt_center_y = (gt_boxes[..., 1] + gt_boxes[..., 3]) / 2 ; print(gt_center_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_box 中心点与 feat_map grid 间的偏置关系\n",
    "# | x - gt_center_x | x方向上的距离\n",
    "# | y - gt_center_y | y方向上的距离\n",
    "c_l_off = x[None, :, None] - gt_center_x[:, None, :]  # [1,h*w,1]-[batch_size,1,m]-->[batch_size,h*w,m]\n",
    "c_t_off = y[None, :, None] - gt_center_y[:, None, :]\n",
    "c_r_off = gt_center_x[:, None, :] - x[None, :, None]\n",
    "c_b_off = gt_center_y[:, None, :] - y[None, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-21.0000, -28.0000,  21.0000,  28.0000],\n",
       "          [-17.5000, -27.0000,  17.5000,  27.0000]],\n",
       "\n",
       "         [[ -5.0000, -28.0000,   5.0000,  28.0000],\n",
       "          [ -1.5000, -27.0000,   1.5000,  27.0000]],\n",
       "\n",
       "         [[ 11.0000, -28.0000, -11.0000,  28.0000],\n",
       "          [ 14.5000, -27.0000, -14.5000,  27.0000]],\n",
       "\n",
       "         [[ 27.0000, -28.0000, -27.0000,  28.0000],\n",
       "          [ 30.5000, -27.0000, -30.5000,  27.0000]],\n",
       "\n",
       "         [[-21.0000, -12.0000,  21.0000,  12.0000],\n",
       "          [-17.5000, -11.0000,  17.5000,  11.0000]],\n",
       "\n",
       "         [[ -5.0000, -12.0000,   5.0000,  12.0000],\n",
       "          [ -1.5000, -11.0000,   1.5000,  11.0000]],\n",
       "\n",
       "         [[ 11.0000, -12.0000, -11.0000,  12.0000],\n",
       "          [ 14.5000, -11.0000, -14.5000,  11.0000]],\n",
       "\n",
       "         [[ 27.0000, -12.0000, -27.0000,  12.0000],\n",
       "          [ 30.5000, -11.0000, -30.5000,  11.0000]],\n",
       "\n",
       "         [[-21.0000,   4.0000,  21.0000,  -4.0000],\n",
       "          [-17.5000,   5.0000,  17.5000,  -5.0000]],\n",
       "\n",
       "         [[ -5.0000,   4.0000,   5.0000,  -4.0000],\n",
       "          [ -1.5000,   5.0000,   1.5000,  -5.0000]],\n",
       "\n",
       "         [[ 11.0000,   4.0000, -11.0000,  -4.0000],\n",
       "          [ 14.5000,   5.0000, -14.5000,  -5.0000]],\n",
       "\n",
       "         [[ 27.0000,   4.0000, -27.0000,  -4.0000],\n",
       "          [ 30.5000,   5.0000, -30.5000,  -5.0000]],\n",
       "\n",
       "         [[-21.0000,  20.0000,  21.0000, -20.0000],\n",
       "          [-17.5000,  21.0000,  17.5000, -21.0000]],\n",
       "\n",
       "         [[ -5.0000,  20.0000,   5.0000, -20.0000],\n",
       "          [ -1.5000,  21.0000,   1.5000, -21.0000]],\n",
       "\n",
       "         [[ 11.0000,  20.0000, -11.0000, -20.0000],\n",
       "          [ 14.5000,  21.0000, -14.5000, -21.0000]],\n",
       "\n",
       "         [[ 27.0000,  20.0000, -27.0000, -20.0000],\n",
       "          [ 30.5000,  21.0000, -30.5000, -21.0000]]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_ltrb_off = torch.stack([c_l_off, c_t_off, c_r_off, c_b_off], dim=-1)  # [batch_size,h*w,m,4]\n",
    "c_ltrb_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[28.0000, 27.0000],\n",
       "         [28.0000, 27.0000],\n",
       "         [28.0000, 27.0000],\n",
       "         [28.0000, 30.5000],\n",
       "         [21.0000, 17.5000],\n",
       "         [12.0000, 11.0000],\n",
       "         [12.0000, 14.5000],\n",
       "         [27.0000, 30.5000],\n",
       "         [21.0000, 17.5000],\n",
       "         [ 5.0000,  5.0000],\n",
       "         [11.0000, 14.5000],\n",
       "         [27.0000, 30.5000],\n",
       "         [21.0000, 21.0000],\n",
       "         [20.0000, 21.0000],\n",
       "         [20.0000, 21.0000],\n",
       "         [27.0000, 30.5000]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_off_max = torch.max(c_ltrb_off, dim=-1)[0]  # 相对于gt_box center 偏的最远的grid\n",
    "c_off_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [ True,  True],\n",
       "         [ True,  True],\n",
       "         [ True,  True],\n",
       "         [False, False],\n",
       "         [ True,  True],\n",
       "         [ True,  True],\n",
       "         [ True,  True],\n",
       "         [False, False],\n",
       "         [ True,  True],\n",
       "         [ True,  True],\n",
       "         [ True,  True],\n",
       "         [False, False]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_center = c_off_max < 1.5 * 16  # 1 * stride / 1.5*stride\n",
    "mask_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/center.png\" width=\"400\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False],\n",
      "         [False, False],\n",
      "         [False, False],\n",
      "         [False, False],\n",
      "         [False, False],\n",
      "         [False,  True],\n",
      "         [False, False],\n",
      "         [False, False],\n",
      "         [False, False],\n",
      "         [ True,  True],\n",
      "         [ True, False],\n",
      "         [False, False],\n",
      "         [False, False],\n",
      "         [False, False],\n",
      "         [False, False],\n",
      "         [False, False]]]) torch.Size([1, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "mask_pos = mask_in_gtboxes & mask_in_level & mask_center\n",
    "print(mask_pos,mask_pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据gt_box面积分配模糊正样本点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于feature map上的各个点（x_fea,y_fea），只要这个点对应的(x_img, y_img)落到了gt bbox(x_min,y_min, x_max,y_max)区域中，那么这个点就是正样本。\n",
    "\n",
    "而如果这个点多在多个bbox中，那个这个点就是**模糊样本**，目前采用面积最小的bbox作为这个点的回归目标。目标框的回归方式也从回归顶点坐标(x, y, x, y)到回归当前点到框的各边的距离(l, t, r, b)。\n",
    "<img src=\"imgs/ancher free.png\" width=\"500\" height=\"400\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.],\n",
       "         [48., 30.]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltrb_off[..., 1] + ltrb_off[..., 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.],\n",
      "         [1824.,  870.]]])\n"
     ]
    }
   ],
   "source": [
    "areas = (ltrb_off[..., 0] + ltrb_off[..., 2]) * (ltrb_off[..., 1] + ltrb_off[..., 3])  # [batch_size,h*w,m]\n",
    "print(areas)  # 框的面积值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999.,   870.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [ 1824.,   870.],\n",
       "         [ 1824., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.],\n",
       "         [99999., 99999.]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areas[~mask_pos] = 99999 # 初始化非样本点\n",
    "areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areas_min_ind = torch.min(areas, dim=-1)[1]  # [batch_size,h*w]\n",
    "areas_min_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areas_min_ind.unsqueeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(areas, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [0, 1],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [0, 1],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [1, 0],\n",
       "         [1, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成独热向量\n",
    "index = torch.zeros_like(areas, dtype=torch.uint8).scatter_(-1, \n",
    "                                                    areas_min_ind.unsqueeze(dim=-1),  # 增加一个维度\n",
    "                                                    1)\n",
    "print(index.shape)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 2, 4])\n",
      "torch.Size([16, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elimen/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2., -4., 40., 52.],\n",
       "        [14., -4., 24., 52.],\n",
       "        [30., -4.,  8., 52.],\n",
       "        [46., -4., -8., 52.],\n",
       "        [-2., 12., 40., 36.],\n",
       "        [13.,  4., 16., 26.],\n",
       "        [30., 12.,  8., 36.],\n",
       "        [46., 12., -8., 36.],\n",
       "        [-2., 28., 40., 20.],\n",
       "        [13., 20., 16., 10.],\n",
       "        [30., 28.,  8., 20.],\n",
       "        [46., 28., -8., 20.],\n",
       "        [-2., 44., 40.,  4.],\n",
       "        [14., 44., 24.,  4.],\n",
       "        [30., 44.,  8.,  4.],\n",
       "        [46., 44., -8.,  4.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定一下面积最小的那个框，特征图上的每个格点对应到它所属的面积最小的gt_box\n",
    "print(ltrb_off.shape)\n",
    "reg_targets = ltrb_off[index]  # [batch_size*h*w, 4]\n",
    "print(reg_targets.shape)\n",
    "reg_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4],\n",
       "         [2, 4]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = torch.broadcast_tensors(classes[:, None, :], areas.long())[0] # areas.long() 转换area的数值类型\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elimen/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = torch.zeros_like(areas, dtype=torch.uint8).scatter_(-1, areas_min_ind.unsqueeze(dim=-1), 1)\n",
    "cls_targets = classes[select]\n",
    "cls_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [cnt_targets 在做什么?](https://blog.csdn.net/shanglianlm/article/details/89007219)\n",
    "**centerness**\n",
    "\n",
    "&emsp;&emsp;通过center-ness来度量当前位置和物体中心间的距离，即FCOS将点的坐标在目标中的位置因素也考虑进来，越靠近中间权重越大。\n",
    "<img src=\"imgs/centerness.jpg\" width=\"500\" height=\"400\" align=\"bottom\">\n",
    "\n",
    "\n",
    "&emsp;&emsp;在训练的过程中通过损失函数，我们会约束center-ness的值，使得其接近于0，使得分布在目标位置边缘的低质量框能够尽可能的靠近中心。在最终使用该网络的过程中，非极大值抑制(NMS)就可以轻松滤除这些低质量的边界框，提高检测性能。\n",
    "<img src=\"imgs/equation3.png\" width=\"500\" height=\"400\" align=\"bottom\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2., 14.,  8., -8., -2., 13.,  8., -8., -2., 13.,  8., -8., -2., 14.,\n",
      "         8., -8.])\n",
      "tensor([40., 24., 30., 46., 40., 16., 30., 46., 40., 16., 30., 46., 40., 24.,\n",
      "        30., 46.])\n"
     ]
    }
   ],
   "source": [
    "left_right_min = torch.min(reg_targets[..., 0], reg_targets[..., 2]) ; print(left_right_min)\n",
    "left_right_max = torch.max(reg_targets[..., 0], reg_targets[..., 2]) ; print(left_right_max)\n",
    "top_bottom_min = torch.min(reg_targets[..., 1], reg_targets[..., 3])\n",
    "top_bottom_max = torch.max(reg_targets[..., 1], reg_targets[..., 3])\n",
    "# 上面的值拿来计算中心目标\n",
    "cnt_targets = ((left_right_min * top_bottom_min) / (left_right_max * top_bottom_max + 1e-4)).sqrt().unsqueeze(\n",
    "    dim=-1)  # [batch_size,h*w,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0620],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.1157],\n",
       "        [   nan],\n",
       "        [0.3536],\n",
       "        [0.2981],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.6374],\n",
       "        [0.4364],\n",
       "        [   nan],\n",
       "        [   nan],\n",
       "        [0.2303],\n",
       "        [0.1557],\n",
       "        [   nan]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据mask制作targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask_pos = mask_in_gtboxes & mask_in_level & mask_center \n",
    "mask_pos.long().sum(dim=-1)  # [batch_size,h*w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pos_2 = mask_pos.long().sum(dim=-1)  # [batch_size,h*w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False,  True, False, False, False,  True,\n",
       "          True, False, False, False, False, False]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos_2 = mask_pos_2 >= 1\n",
    "mask_pos_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0])\n",
      "tensor([[-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.3536],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [ 0.6374],\n",
      "        [ 0.4364],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000],\n",
      "        [-1.0000]])\n",
      "tensor([[-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [13.,  4., 16., 26.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [13., 20., 16., 10.],\n",
      "        [30., 28.,  8., 20.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# 对于只有一个gt_box的情况\n",
    "cls_targets[~mask_pos_2.squeeze()] = 0   # [batch_size,h*w,1]\n",
    "cnt_targets[~mask_pos_2.squeeze()] = -1  \n",
    "reg_targets[~mask_pos_2.squeeze()] = -1\n",
    "\n",
    "print(cls_targets)\n",
    "print(cnt_targets)\n",
    "print(reg_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总体函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "  def _gen_level_targets(out, gt_boxes, classes, stride, limit_range, sample_radiu_ratio=1.5):\n",
    "        '''\n",
    "        Args  \n",
    "        out list contains [[batch_size,class_num,h,w],[batch_size,1,h,w],[batch_size,4,h,w]]  \n",
    "        gt_boxes [batch_size,m,4]  \n",
    "        classes [batch_size,m]  \n",
    "        stride int  \n",
    "        limit_range list [min,max]  \n",
    "        Returns  \n",
    "        cls_targets,cnt_targets,reg_targets\n",
    "        '''\n",
    "        # 1 三个同一层的分类 中心 定位的 预测数据\n",
    "        cls_logits, cnt_logits, reg_preds = out\n",
    "        batch_size = cls_logits.shape[0]\n",
    "        class_num = cls_logits.shape[1]\n",
    "        # 2 一个图片有多少个标签框\n",
    "        m = gt_boxes.shape[1]\n",
    "        # 3 reshape (batch_size, h*w, n)\n",
    "        cls_logits = cls_logits.permute(0, 2, 3, 1)  # [batch_size,h,w,class_num]\n",
    "        coords = coords_fmap2orig(cls_logits, stride).to(device=gt_boxes.device)  # [h*w,2]\n",
    "\n",
    "        cls_logits = cls_logits.reshape((batch_size, -1, class_num))  # [batch_size,h*w,class_num]\n",
    "        # 4 reshape cnt\n",
    "        cnt_logits = cnt_logits.permute(0, 2, 3, 1)   \n",
    "        cnt_logits = cnt_logits.reshape((batch_size, -1, 1))    # [batch_size,h*w,1]\n",
    "        # 5 reshape reg\n",
    "        reg_preds = reg_preds.permute(0, 2, 3, 1)\n",
    "        reg_preds = reg_preds.reshape((batch_size, -1, 4))  #  # [batch_size,h*w,4]\n",
    "\n",
    "        h_mul_w = cls_logits.shape[1]  # h*w\n",
    "\n",
    "        x = coords[:, 0]\n",
    "        y = coords[:, 1]\n",
    "        # 6 判断gt_box 包含了特征图上哪些grid(格点)\n",
    "        l_off = x[None, :, None] - gt_boxes[..., 0][:, None, :]  # [1,h*w,1]-[batch_size,1,m]-->[batch_size,h*w,m]\n",
    "        t_off = y[None, :, None] - gt_boxes[..., 1][:, None, :]\n",
    "        \n",
    "        r_off = gt_boxes[..., 2][:, None, :] - x[None, :, None]\n",
    "        b_off = gt_boxes[..., 3][:, None, :] - y[None, :, None]\n",
    "        \n",
    "        ltrb_off = torch.stack([l_off, t_off, r_off, b_off], dim=-1)  # [batch_size,h*w,m,4]\n",
    "\n",
    "        areas = (ltrb_off[..., 0] + ltrb_off[..., 2]) * (ltrb_off[..., 1] + ltrb_off[..., 3])  # [batch_size,h*w,m]\n",
    "\n",
    "        \n",
    "        off_min = torch.min(ltrb_off, dim=-1)[0]  # [batch_size,h*w,m]\n",
    "        off_max = torch.max(ltrb_off, dim=-1)[0]  # [batch_size,h*w,m]\n",
    "\n",
    "        mask_in_gtboxes = off_min > 0  # feature map 上的gird 是不是在框里\n",
    "        mask_in_level = (off_max > limit_range[0]) & (off_max <= limit_range[1])  # 这个框是不是在这一层里\n",
    "        # 7 找到离gt box 不远（1*stride）的grid\n",
    "        radiu = stride * sample_radiu_ratio\n",
    "        gt_center_x = (gt_boxes[..., 0] + gt_boxes[..., 2]) / 2\n",
    "        gt_center_y = (gt_boxes[..., 1] + gt_boxes[..., 3]) / 2\n",
    "        \n",
    "        c_l_off = x[None, :, None] - gt_center_x[:, None, :]  # [1,h*w,1]-[batch_size,1,m]-->[batch_size,h*w,m]\n",
    "        c_t_off = y[None, :, None] - gt_center_y[:, None, :]\n",
    "        c_r_off = gt_center_x[:, None, :] - x[None, :, None]\n",
    "        c_b_off = gt_center_y[:, None, :] - y[None, :, None]\n",
    "        \n",
    "        c_ltrb_off = torch.stack([c_l_off, c_t_off, c_r_off, c_b_off], dim=-1)  # [batch_size,h*w,m,4]\n",
    "        c_off_max = torch.max(c_ltrb_off, dim=-1)[0]\n",
    "        mask_center = c_off_max < radiu\n",
    "        # 选择 满足条件的grid\n",
    "        mask_pos = mask_in_gtboxes & mask_in_level & mask_center  # [batch_size,h*w,m]\n",
    "\n",
    "        areas[~mask_pos] = 99999999\n",
    "        areas_min_ind = torch.min(areas, dim=-1)[1]  # [batch_size,h*w]\n",
    "        reg_targets = ltrb_off[torch.zeros_like(areas, dtype=torch.uint8).scatter_(-1, areas_min_ind.unsqueeze(dim=-1),\n",
    "                                                                                   1)]  # [batch_size*h*w,4]\n",
    "        reg_targets = torch.reshape(reg_targets, (batch_size, -1, 4))  # [batch_size,h*w,4]\n",
    "\n",
    "        classes = torch.broadcast_tensors(classes[:, None, :], areas.long())[0]  # [batch_size,h*w,m]\n",
    "        cls_targets = classes[\n",
    "            torch.zeros_like(areas, dtype=torch.uint8).scatter_(-1, areas_min_ind.unsqueeze(dim=-1), 1)]\n",
    "        cls_targets = torch.reshape(cls_targets, (batch_size, -1, 1))  # [batch_size,h*w,1]\n",
    "\n",
    "        left_right_min = torch.min(reg_targets[..., 0], reg_targets[..., 2])  # [batch_size,h*w]\n",
    "        left_right_max = torch.max(reg_targets[..., 0], reg_targets[..., 2])\n",
    "        top_bottom_min = torch.min(reg_targets[..., 1], reg_targets[..., 3])\n",
    "        top_bottom_max = torch.max(reg_targets[..., 1], reg_targets[..., 3])\n",
    "        cnt_targets = ((left_right_min * top_bottom_min) / (left_right_max * top_bottom_max + 1e-10)).sqrt().unsqueeze(\n",
    "            dim=-1)  # [batch_size,h*w,1]\n",
    "\n",
    "        assert reg_targets.shape == (batch_size, h_mul_w, 4)\n",
    "        assert cls_targets.shape == (batch_size, h_mul_w, 1)\n",
    "        assert cnt_targets.shape == (batch_size, h_mul_w, 1)\n",
    "\n",
    "        # process neg coords\n",
    "        mask_pos_2 = mask_pos.long().sum(dim=-1)  # [batch_size,h*w]\n",
    "        # num_pos=mask_pos_2.sum(dim=-1)\n",
    "        # assert num_pos.shape==(batch_size,)\n",
    "        mask_pos_2 = mask_pos_2 >= 1  # 再确认一下\n",
    "        assert mask_pos_2.shape == (batch_size, h_mul_w)\n",
    "        cls_targets[~mask_pos_2] = 0   # [batch_size,h*w,1]\n",
    "        cnt_targets[~mask_pos_2] = -1  \n",
    "        reg_targets[~mask_pos_2] = -1\n",
    "\n",
    "        return cls_targets, cnt_targets, reg_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
